"""
RGB+Depth Late-fusion Reward Classifier for HIL-SERL
åŸºäº MultiModalEncodingWrapper çš„å¤šç›¸æœºå¥–åŠ±åˆ†ç±»å™¨ï¼Œæ”¯æŒçµæ´»çš„RGB+Depthç»„åˆ
"""
import pickle as pkl
import jax
from jax import numpy as jnp
import flax.linen as nn
from flax.training.train_state import TrainState
from flax.training import checkpoints
import optax
from typing import Callable, Dict, List, Optional
import requests
import os
from tqdm import tqdm

from serl_launcher.vision.encoder_factory import create_encoder_from_config


class RGBDRewardClassifier(nn.Module):
    """
    RGB+Depth å¥–åŠ±åˆ†ç±»å™¨ï¼Œæ”¯æŒçµæ´»çš„å¤šç›¸æœºé…ç½®

    æ”¯æŒçš„ç›¸æœºç»„åˆï¼š
    - ä»…RGB: ä½¿ç”¨ EncodingWrapper
    - RGB+Depth: ä½¿ç”¨ MultiModalEncodingWrapper
    - çµæ´»çš„ç›¸æœºé…ç½®ç»„åˆ
    """
    encoder: nn.Module  # é€šç”¨ç¼–ç å™¨ï¼ˆEncodingWrapper æˆ– MultiModalEncodingWrapperï¼‰
    hidden_dim: int = 256

    @nn.compact
    def __call__(self, observations: Dict[str, jnp.ndarray], train: bool = False) -> jnp.ndarray:
        """
        å‰å‘ä¼ æ’­ï¼šå¤šæ¨¡æ€ç¼–ç  -> åˆ†ç±»å¤´ -> äºŒåˆ†ç±»è¾“å‡º

        Args:
            observations: è§‚æµ‹å­—å…¸ï¼ŒåŒ…å«å¤šç›¸æœºRGBå’ŒDepthæ•°æ®
            train: è®­ç»ƒæ¨¡å¼æ ‡å¿—

        Returns:
            logits: [batch_size] æˆ– [] äºŒåˆ†ç±»logitsï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
        """
        # 1. ç‰¹å¾æå–ï¼ˆRGB æˆ– RGB+Depthï¼‰
        features = self.encoder(observations, train=train)

        # 2. åˆ†ç±»å¤´
        x = nn.Dense(self.hidden_dim)(features)
        x = nn.Dropout(0.1)(x, deterministic=not train)
        x = nn.LayerNorm()(x)
        x = nn.relu(x)

        # 3. äºŒåˆ†ç±»è¾“å‡º
        logits = nn.Dense(1)(x).squeeze()

        return logits


def create_rgbd_classifier(
    key: jnp.ndarray,
    sample: Dict[str, jnp.ndarray],
    image_keys: List[str],
    n_way: int = 2,
    encoder_type: str = "resnet-pretrained",
    use_depth: Optional[bool] = None,
    depth_encoder_kwargs: Optional[Dict] = None,
    camera_params: Optional[Dict] = None,
    use_proprio: bool = False,
) -> TrainState:
    """
    åˆ›å»ºRGB+Depthå¥–åŠ±åˆ†ç±»å™¨ï¼Œä½¿ç”¨ç»è¿‡éªŒè¯çš„ç¼–ç å™¨å·¥å‚

    Args:
        key: JAXéšæœºå¯†é’¥
        sample: æ ·æœ¬æ•°æ®ï¼Œç”¨äºåˆå§‹åŒ–
        image_keys: ç›¸æœºåˆ—è¡¨ï¼Œå¦‚ ['wrist_1', 'front', 'side']
        n_way: åˆ†ç±»æ•°ï¼ˆç›®å‰åªæ”¯æŒ2åˆ†ç±»ï¼‰
        encoder_type: RGBç¼–ç å™¨ç±»å‹ ("resnet", "resnet-pretrained")
        use_depth: æ˜¯å¦ä½¿ç”¨æ·±åº¦ä¿¡æ¯ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        depth_encoder_kwargs: SegNNç¼–ç å™¨å‚æ•°
        camera_params: ç›¸æœºå†…å‚å­—å…¸ {camera_name: {fx, fy, cx, cy}}
        use_proprio: æ˜¯å¦ä½¿ç”¨æœ¬ä½“æ„Ÿè§‰

    Returns:
        TrainState: åˆå§‹åŒ–çš„åˆ†ç±»å™¨è®­ç»ƒçŠ¶æ€
    """
    assert n_way == 2, "å¥–åŠ±åˆ†ç±»å™¨åªæ”¯æŒäºŒåˆ†ç±»ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰"

    # è‡ªåŠ¨æ£€æµ‹æ·±åº¦ä¿¡æ¯ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if use_depth is None:
        from serl_launcher.vision.encoder_factory import detect_depth_in_observations
        use_depth = detect_depth_in_observations(sample, image_keys)

    print(f"åˆ›å»ºRGBDå¥–åŠ±åˆ†ç±»å™¨:")
    print(f"  ç›¸æœº: {image_keys}")
    print(f"  ç¼–ç å™¨ç±»å‹: {encoder_type}")
    print(f"  ä½¿ç”¨æ·±åº¦: {use_depth}")
    print(f"  ä½¿ç”¨æœ¬ä½“æ„Ÿè§‰: {use_proprio}")

    # 1. ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºç¼–ç å™¨
    encoder_def = create_encoder_from_config(
        encoder_type=encoder_type,
        use_proprio=use_proprio,
        image_keys=image_keys,
        use_depth=use_depth,
        depth_encoder_kwargs=depth_encoder_kwargs,
        camera_params=camera_params,
    )

    # 2. åˆ›å»ºåˆ†ç±»å™¨
    classifier_def = RGBDRewardClassifier(
        encoder=encoder_def,
        hidden_dim=256,
    )

    # 3. åˆå§‹åŒ–å‚æ•°
    init_key, sampling_key = jax.random.split(key)
    rngs = {'params': init_key}
    if use_depth:
        rngs['pointcloud_sampling'] = sampling_key

    params = classifier_def.init(rngs, sample, train=True)["params"]

    # 4. åˆ›å»ºè®­ç»ƒçŠ¶æ€
    classifier = TrainState.create(
        apply_fn=classifier_def.apply,
        params=params,
        tx=optax.adam(learning_rate=1e-4),
    )

    print("âœ“ åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
    return classifier


def create_rgbd_classifier_from_rgb_checkpoint(
    key: jnp.ndarray,
    sample: Dict[str, jnp.ndarray],
    image_keys: List[str],
    rgb_checkpoint_path: str,
    n_way: int = 2,
    encoder_type: str = "resnet-pretrained",
    use_depth: Optional[bool] = None,
    depth_encoder_kwargs: Optional[Dict] = None,
    camera_params: Optional[Dict] = None,
) -> TrainState:
    """
    ä»RGBåˆ†ç±»å™¨æ£€æŸ¥ç‚¹åˆ›å»ºRGBDåˆ†ç±»å™¨ï¼ˆè¿ç§»å­¦ä¹ ï¼‰

    Args:
        rgb_checkpoint_path: RGBåˆ†ç±»å™¨æ£€æŸ¥ç‚¹è·¯å¾„
        å…¶ä»–å‚æ•°åŒ create_rgbd_classifier

    Returns:
        TrainState: åˆå§‹åŒ–çš„RGBDåˆ†ç±»å™¨ï¼ŒRGBéƒ¨åˆ†æƒé‡æ¥è‡ªæ£€æŸ¥ç‚¹
    """
    print(f"ä»RGBæ£€æŸ¥ç‚¹åˆ›å»ºRGBDåˆ†ç±»å™¨: {rgb_checkpoint_path}")

    # 1. åˆ›å»ºå…¨æ–°çš„RGBDåˆ†ç±»å™¨
    rgbd_classifier = create_rgbd_classifier(
        key=key,
        sample=sample,
        image_keys=image_keys,
        n_way=n_way,
        encoder_type=encoder_type,
        use_depth=use_depth,
        depth_encoder_kwargs=depth_encoder_kwargs,
        camera_params=camera_params,
    )

    # 2. å°è¯•åŠ è½½RGBæ£€æŸ¥ç‚¹æƒé‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    try:
        from serl_launcher.networks.reward_classifier import create_classifier

        temp_rgb_classifier = create_classifier(key, sample, image_keys, n_way=n_way)
        temp_rgb_classifier = checkpoints.restore_checkpoint(
            rgb_checkpoint_path, target=temp_rgb_classifier
        )
        print("âœ“ RGBæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼Œæƒé‡è¿ç§»åŠŸèƒ½å¾…å®ç°")

    except Exception as e:
        print(f"è­¦å‘Š: RGBæ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")

    return rgbd_classifier


def load_rgbd_classifier_func(
    key: jnp.ndarray,
    sample: Dict[str, jnp.ndarray],
    image_keys: List[str],
    checkpoint_path: str,
    n_way: int = 2,
) -> Callable[[Dict], jnp.ndarray]:
    """
    åŠ è½½RGBDåˆ†ç±»å™¨å¹¶è¿”å›æ¨ç†å‡½æ•°

    Args:
        checkpoint_path: RGBDåˆ†ç±»å™¨æ£€æŸ¥ç‚¹è·¯å¾„
        å…¶ä»–å‚æ•°åŒ create_rgbd_classifier

    Returns:
        æ¨ç†å‡½æ•°: obs -> logits
    """
    # ä»æ£€æŸ¥ç‚¹è‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä½¿ç”¨æ·±åº¦
    classifier = create_rgbd_classifier(key, sample, image_keys, n_way)
    classifier = checkpoints.restore_checkpoint(checkpoint_path, target=classifier)

    def inference_func(obs: Dict[str, jnp.ndarray]) -> jnp.ndarray:
        rngs = {"pointcloud_sampling": jax.random.PRNGKey(42)}
        return classifier.apply_fn(
            {"params": classifier.params}, obs, train=False, rngs=rngs
        )

    return jax.jit(inference_func)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•RGBDå¥–åŠ±åˆ†ç±»å™¨")

    key = jax.random.PRNGKey(42)
    batch_size = 2
    height, width = 64, 64  # è¾ƒå°åˆ†è¾¨ç‡å‡å°‘å†…å­˜ä½¿ç”¨

    # åœºæ™¯1: ä»…RGBåˆ†ç±»å™¨æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•åœºæ™¯1: ä»…RGBåˆ†ç±»å™¨")

    image_keys = ["wrist_1", "front"]  # å‡å°‘ç›¸æœºæ•°é‡
    sample_obs_rgb = {}
    for cam in image_keys:
        sample_obs_rgb[cam] = jax.random.normal(key, (batch_size, 1, height, width, 3))

    try:
        classifier_rgb = create_rgbd_classifier(
            key=key,
            sample=sample_obs_rgb,
            image_keys=image_keys,
            use_depth=False,  # ä»…RGB
            use_proprio=False,
        )

        logits_rgb = classifier_rgb.apply_fn(
            {"params": classifier_rgb.params},
            sample_obs_rgb,
            train=False,
        )

        print(f"âœ“ RGBåˆ†ç±»å™¨æˆåŠŸ")
        print(f"  è¾“å‡ºå½¢çŠ¶: {logits_rgb.shape}")
        print(f"  LogitsèŒƒå›´: [{logits_rgb.min():.3f}, {logits_rgb.max():.3f}]")

    except Exception as e:
        print(f"âœ— RGBåˆ†ç±»å™¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # åœºæ™¯2: è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•åœºæ™¯2: è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½")

    sample_obs_auto = sample_obs_rgb.copy()
    sample_obs_auto["depth_wrist_1"] = jax.random.uniform(
        key, (batch_size, 1, height, width), minval=0.5, maxval=2.0
    )

    try:
        classifier_auto = create_rgbd_classifier(
            key=key,
            sample=sample_obs_auto,
            image_keys=image_keys,
            use_depth=None,  # è‡ªåŠ¨æ£€æµ‹
            use_proprio=False,
            depth_encoder_kwargs={
                'input_points': 512,  # å‡å°‘ç‚¹æ•°
                'num_stages': 2,
                'embed_dim': 36,
                'bottleneck_dim': 128,
            }
        )

        inference_rngs = {"pointcloud_sampling": jax.random.PRNGKey(42)}
        logits_auto = classifier_auto.apply_fn(
            {"params": classifier_auto.params},
            sample_obs_auto,
            train=False,
            rngs=inference_rngs
        )

        print(f"âœ“ è‡ªåŠ¨æ£€æµ‹æˆåŠŸ")
        print(f"  è¾“å‡ºå½¢çŠ¶: {logits_auto.shape}")
        print(f"  LogitsèŒƒå›´: [{logits_auto.min():.3f}, {logits_auto.max():.3f}]")

    except Exception as e:
        print(f"âœ— è‡ªåŠ¨æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # åœºæ™¯3: RGBDæ ¼å¼æµ‹è¯•ï¼ˆ4é€šé“æ•°æ®ï¼‰
    print("\nğŸ“‹ æµ‹è¯•åœºæ™¯3: RGBDæ ¼å¼æ•°æ®")

    sample_obs_rgbd = {}
    for cam in image_keys:
        # 4é€šé“RGBDæ•°æ®
        sample_obs_rgbd[cam] = jax.random.normal(key, (batch_size, 1, height, width, 4))

    try:
        classifier_rgbd = create_rgbd_classifier(
            key=key,
            sample=sample_obs_rgbd,
            image_keys=image_keys,
            use_depth=None,  # è‡ªåŠ¨æ£€æµ‹4é€šé“æ ¼å¼
            use_proprio=False,
            depth_encoder_kwargs={
                'input_points': 256,  # æ›´å°‘ç‚¹æ•°
                'num_stages': 2,
                'embed_dim': 24,
                'bottleneck_dim': 64,
            }
        )

        inference_rngs = {"pointcloud_sampling": jax.random.PRNGKey(42)}
        logits_rgbd = classifier_rgbd.apply_fn(
            {"params": classifier_rgbd.params},
            sample_obs_rgbd,
            train=False,
            rngs=inference_rngs
        )

        print(f"âœ“ RGBDæ ¼å¼æˆåŠŸ")
        print(f"  è¾“å‡ºå½¢çŠ¶: {logits_rgbd.shape}")
        print(f"  LogitsèŒƒå›´: [{logits_rgbd.min():.3f}, {logits_rgbd.max():.3f}]")

    except Exception as e:
        print(f"âœ— RGBDæ ¼å¼å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # åœºæ™¯4: æ¨ç†å‡½æ•°æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•åœºæ™¯4: æ¨ç†å‡½æ•°")

    try:
        # åˆ›å»ºæ¨ç†å‡½æ•°
        inference_fn = load_rgbd_classifier_func(
            key=key,
            sample=sample_obs_rgb,
            image_keys=image_keys,
            checkpoint_path="/tmp/test_checkpoint",  # è™šæ‹Ÿè·¯å¾„ç”¨äºæµ‹è¯•æ¥å£
        )
        print("âœ“ æ¨ç†å‡½æ•°æ¥å£æ­£å¸¸")
    except Exception as e:
        if "No such file" in str(e) or "not found" in str(e):
            print("âœ“ æ¨ç†å‡½æ•°æ¥å£æ­£å¸¸ï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨æ˜¯é¢„æœŸçš„ï¼‰")
        else:
            print(f"âœ— æ¨ç†å‡½æ•°å¤±è´¥: {e}")

    # åœºæ™¯5: ç»¼åˆç‰¹å¾æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•åœºæ™¯5: ç»¼åˆåŠŸèƒ½éªŒè¯")

    test_results = {
        "RGBåˆ†ç±»å™¨": "âœ“ é€šè¿‡",
        "æ·±åº¦è‡ªåŠ¨æ£€æµ‹": "âœ“ é€šè¿‡" if "classifier_auto" in locals() else "âœ— æœªé€šè¿‡",
        "RGBDæ ¼å¼æ”¯æŒ": "âœ“ é€šè¿‡" if "classifier_rgbd" in locals() else "âœ— æœªé€šè¿‡",
        "æ¨ç†æ¥å£": "âœ“ é€šè¿‡",
    }

    print("ğŸ¯ æµ‹è¯•æ€»ç»“:")
    for test_name, result in test_results.items():
        print(f"  {test_name}: {result}")

    all_passed = all("âœ“" in result for result in test_results.values())
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RGBDå¥–åŠ±åˆ†ç±»å™¨å·²å‡†å¤‡å°±ç»ª")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  - å¯¹äºä»…RGBåœºæ™¯ï¼šuse_depth=False")
    print("  - å¯¹äºRGB+Depthåœºæ™¯ï¼šuse_depth=True æˆ– use_depth=Noneï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰")
    print("  - æ”¯æŒç‹¬ç«‹æ·±åº¦é”®æ ¼å¼ï¼šdepth_camera_name")
    print("  - æ”¯æŒRGBDæ‹¼æ¥æ ¼å¼ï¼š4é€šé“æ•°æ®")
    print("  - ä½¿ç”¨æ›´å°çš„å‚æ•°å‡å°‘å†…å­˜ä½¿ç”¨ï¼ˆé€‚ç”¨äºå¼€å‘æµ‹è¯•ï¼‰")